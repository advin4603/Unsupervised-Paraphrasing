# Unsupervised-Paraphrasing

This repository features an unsupervised paraphrasing method using a Seq2Seq model (e.g., GPT-2). The model is trained by corrupting input sentences through techniques like synonym substitution, stopword removal and phrase order shuffling, aiming to generate diverse and novel paraphrases without relying on labeled data. The repository includes code for training and using the paraphraser.
